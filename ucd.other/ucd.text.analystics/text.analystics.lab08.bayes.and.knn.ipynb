{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COMP30810 - INTRO TO TEXT ANALYTICS\n",
    "> LECTURER: BINH THANH LE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Synthetic-dataset\" data-toc-modified-id=\"Synthetic-dataset-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Synthetic dataset</a></span><ul class=\"toc-item\"><li><span><a href=\"#Data-Preprocessing\" data-toc-modified-id=\"Data-Preprocessing-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Data Preprocessing</a></span><ul class=\"toc-item\"><li><span><a href=\"#RAW-data:\" data-toc-modified-id=\"RAW-data:-1.1.1\"><span class=\"toc-item-num\">1.1.1&nbsp;&nbsp;</span>RAW data:</a></span></li><li><span><a href=\"#Raw---&gt;-Clean-Tokens\" data-toc-modified-id=\"Raw--->-Clean-Tokens-1.1.2\"><span class=\"toc-item-num\">1.1.2&nbsp;&nbsp;</span>Raw --&gt; Clean Tokens</a></span></li><li><span><a href=\"#Tokens---&gt;-TFIDF\" data-toc-modified-id=\"Tokens--->-TFIDF-1.1.3\"><span class=\"toc-item-num\">1.1.3&nbsp;&nbsp;</span>Tokens --&gt; TFIDF</a></span></li></ul></li><li><span><a href=\"#Data-Modeling\" data-toc-modified-id=\"Data-Modeling-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Data Modeling</a></span></li><li><span><a href=\"#Data-Applying\" data-toc-modified-id=\"Data-Applying-1.3\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>Data Applying</a></span></li></ul></li><li><span><a href=\"#Real-Dataset:-20-Newsgoup\" data-toc-modified-id=\"Real-Dataset:-20-Newsgoup-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Real Dataset: 20 Newsgoup</a></span><ul class=\"toc-item\"><li><span><a href=\"#Step-1:-Loading-the-data-set-in-jupyter.\" data-toc-modified-id=\"Step-1:-Loading-the-data-set-in-jupyter.-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Step 1: Loading the data set in jupyter.</a></span></li><li><span><a href=\"#Step-2:-Extracting-features-from-text-files.\" data-toc-modified-id=\"Step-2:-Extracting-features-from-text-files.-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Step 2: Extracting features from text files.</a></span></li><li><span><a href=\"#Step-3.-Running-ML-algorithms\" data-toc-modified-id=\"Step-3.-Running-ML-algorithms-2.3\"><span class=\"toc-item-num\">2.3&nbsp;&nbsp;</span>Step 3. Running ML algorithms</a></span></li></ul></li><li><span><a href=\"#Hyperparameter-Optimization\" data-toc-modified-id=\"Hyperparameter-Optimization-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span><span class=\"mark\">Hyperparameter Optimization</span></a></span><ul class=\"toc-item\"><li><span><a href=\"#For-Bayes-Classifier\" data-toc-modified-id=\"For-Bayes-Classifier-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>For Bayes Classifier</a></span></li><li><span><a href=\"#For-KNN-Classifier\" data-toc-modified-id=\"For-KNN-Classifier-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>For KNN Classifier</a></span></li></ul></li><li><span><a href=\"#Final-Learning\" data-toc-modified-id=\"Final-Learning-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Final Learning</a></span><ul class=\"toc-item\"><li><span><a href=\"#Recheck-with-other-parameter's-setup\" data-toc-modified-id=\"Recheck-with-other-parameter's-setup-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>Recheck with other parameter's setup</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-04T18:59:34.505707Z",
     "start_time": "2018-11-04T18:59:32.523056Z"
    }
   },
   "outputs": [],
   "source": [
    "# Importing libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import os, io, nltk\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.corpus import stopwords \n",
    "import collections\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import string\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Synthetic dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RAW data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-04T18:59:34.523700Z",
     "start_time": "2018-11-04T18:59:34.509337Z"
    }
   },
   "outputs": [],
   "source": [
    "train_raw = ['Cricket is a bat and ball game played between two teams of eleven players each on a cricket field.',\n",
    "'Each phase of play is called an innings during which one team bats, attempting to score as many runs as possible.',\n",
    "'The teams have one or two innings apiece and, when the first innings ends, the teams swap roles for the next innings',\n",
    "'Before a match begins, the two team captains meet on the pitch for the toss of a coin to determine which team will bat first.',\n",
    "'Two batsmen and eleven fielders then enter the field and play begins when a member of the fielding team, known as the bowler, delivers the ball.',\n",
    "'The most common dismissal in cricket match are bowled, when the bowler hits the stumps directly with the ball and dislodges the bails. Batsman gets out.',\n",
    "'Runs are scored by two main methods: either by hitting the ball hard enough for it to cross the boundary, or by the two batsmen swapping ends.',\n",
    "'The main objective of each team is to score more runs than their opponents.',\n",
    "'If the team batting last is all out having scored fewer runs than their opponents, they are said to have \"lost by n runs\".',\n",
    "' The role of striker batsman is to prevent the ball from hitting the stumps by using his bat and, simultaneously, to strike it well enough to score runs',\n",
    "' Artificial intelligence is intelligence exhibited by machines, rather than humans or other animals. ',\n",
    "' the field of AI research defines itself as the study of \"intelligent agents\": any device that perceives its environment and takes actions that maximize its chance of success at some goal',\n",
    "' The overall research goal of artificial intelligence is to create technology that allows computers and machines to function in an intelligent manner.',\n",
    "' Natural language processing[77] gives machines the ability to read and understand human language and extract intelligence from it.',\n",
    "' AI researchers developed sophisticated mathematical tools to solve specific subproblems. These tools are truly scientific, in the sense that their results are both measurable and verifiable.',\n",
    "' An intelligent agent is a system that perceives its environment and takes actions which maximize its chances of success.',\n",
    "' AI techniques have become an essential part of the technology industry, helping to solve many challenging problems in computer science.',\n",
    "' Recent advancements in AI, and specifically in machine learning, have contributed to the growth of Autonomous Things such as drones and self-driving cars.',\n",
    "' AI research was revived by the commercial success of expert systems,[28] a form of AI program that simulated the knowledge and analytical skills of human experts.',\n",
    "' Advanced statistical techniques (loosely known as deep learning), access to large amounts of data and faster computers enabled advances in machine learning and perception.',\n",
    "' A compound is a pure chemical substance composed of more than one element and the properties of a compound bear little similarity to those of its elements.',\n",
    "' Since the properties of an element are mostly determined by its electron configuration, the properties of the elements likewise show recurring patterns or periodic behaviour.',\n",
    "' The property of inertness of noble gases makes them very suitable in chemicals where reactions are not wanted.',\n",
    "' The atom is also the smallest entity that can be envisaged to retain the chemical properties of the element, such as electronegativity, ionization potential and preferred oxidation state.',\n",
    "' The nucleus is made up of positively charged protons and uncharged neutrons (together called nucleons), while the electron cloud consists of negatively charged electrons which orbit the nucleus',\n",
    "' The atom is the basic unit of chemistry. It consists of a dense core called the atomic nucleus surrounded by a space called the electron cloud.',\n",
    "' A chemical reaction is a transformation of some substances into one or more different substances.',\n",
    "' Chemistry is sometimes called the central science because it bridges other natural sciences, including physics, geology and biology.',\n",
    "' Chemistry includes topics such as the properties of individual atoms and how atoms form chemical bonds to create chemical compounds.',\n",
    "' Chemistry is a branch of physical science that studies the composition, structure of atoms, properties and change of matter.']\n",
    "\n",
    "# Creating true labels for 30 training sentences: {0:Cricket ; 1:AI ; 2:Chemistry}\n",
    "train_label = np.zeros(30)\n",
    "train_label[10:20] = 1\n",
    "train_label[20:30] = 2\n",
    "\n",
    "\n",
    "# Test data\n",
    "test_raw = [\"Chemical compunds are used for preparing bombs based on some reactions\",\\\n",
    "                  \"Cricket is a boring game where the batsman only enjoys the game\",\\\n",
    "                  \"Machine learning is a area of Artificial intelligence\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Raw --> Clean Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-04T18:59:34.538469Z",
     "start_time": "2018-11-04T18:59:34.527266Z"
    }
   },
   "outputs": [],
   "source": [
    "stop = set(stopwords.words('english'))\n",
    "exclude = set(string.punctuation) \n",
    "lemma = WordNetLemmatizer()\n",
    "\n",
    "# Cleaning the text sentences so that punctuation marks, stop words & digits are removed  \n",
    "# Raw --> tokens --> remove stopwords --> remove punctuation --> Lemmatization --> remove number\n",
    "def clean(doc):\n",
    "    stop_free = \" \".join([i for i in doc.lower().split() if i not in stop])\n",
    "    punc_free = ''.join(ch for ch in stop_free if ch not in exclude)\n",
    "    normalized = \" \".join(lemma.lemmatize(word) for word in punc_free.split())\n",
    "    processed = re.sub(r\"\\d+\",\"\",normalized)\n",
    "    cleaned_data = processed.split()\n",
    "    return cleaned_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-04T18:59:36.825641Z",
     "start_time": "2018-11-04T18:59:34.541443Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 30 sentences along with 3 classes : \n",
      "1. Cricket \n",
      "2. Artificial Intelligence \n",
      "3. Chemistry\n",
      "----- Cleaning Done! -----\n"
     ]
    }
   ],
   "source": [
    "print(\"There are 30 sentences along with 3 classes : \\n1. Cricket \\n2. Artificial Intelligence \\n3. Chemistry\")\n",
    "train_clean_data = []\n",
    "\n",
    "for line in train_raw:\n",
    "    line = line.strip()\n",
    "    cleaned = clean(line)\n",
    "    cleaned = ' '.join(cleaned)\n",
    "    train_clean_data.append(cleaned)\n",
    "print('----- Cleaning Done! -----')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokens --> TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-04T18:59:36.850080Z",
     "start_time": "2018-11-04T18:59:36.828803Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<30x223 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 327 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(stop_words='english')\n",
    "train_data = tfidf_vectorizer.fit_transform(train_clean_data)\n",
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-04T18:59:36.920281Z",
     "start_time": "2018-11-04T18:59:36.853147Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>ability</th>\n",
       "      <th>access</th>\n",
       "      <th>action</th>\n",
       "      <th>advance</th>\n",
       "      <th>advanced</th>\n",
       "      <th>advancement</th>\n",
       "      <th>agent</th>\n",
       "      <th>ai</th>\n",
       "      <th>allows</th>\n",
       "      <th>analytical</th>\n",
       "      <th>...</th>\n",
       "      <th>topic</th>\n",
       "      <th>toss</th>\n",
       "      <th>transformation</th>\n",
       "      <th>truly</th>\n",
       "      <th>uncharged</th>\n",
       "      <th>understand</th>\n",
       "      <th>unit</th>\n",
       "      <th>using</th>\n",
       "      <th>verifiable</th>\n",
       "      <th>wanted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.320343</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.313317</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 223 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  ability access action advance advanced advancement agent   ai allows  \\\n",
       "0     0.0    0.0    0.0     0.0      0.0         0.0   0.0  0.0    0.0   \n",
       "1     0.0    0.0    0.0     0.0      0.0         0.0   0.0  0.0    0.0   \n",
       "2     0.0    0.0    0.0     0.0      0.0         0.0   0.0  0.0    0.0   \n",
       "3     0.0    0.0    0.0     0.0      0.0         0.0   0.0  0.0    0.0   \n",
       "4     0.0    0.0    0.0     0.0      0.0         0.0   0.0  0.0    0.0   \n",
       "5     0.0    0.0    0.0     0.0      0.0         0.0   0.0  0.0    0.0   \n",
       "6     0.0    0.0    0.0     0.0      0.0         0.0   0.0  0.0    0.0   \n",
       "7     0.0    0.0    0.0     0.0      0.0         0.0   0.0  0.0    0.0   \n",
       "8     0.0    0.0    0.0     0.0      0.0         0.0   0.0  0.0    0.0   \n",
       "9     0.0    0.0    0.0     0.0      0.0         0.0   0.0  0.0    0.0   \n",
       "\n",
       "  analytical  ...   topic      toss transformation truly uncharged understand  \\\n",
       "0        0.0  ...     0.0  0.000000            0.0   0.0       0.0        0.0   \n",
       "1        0.0  ...     0.0  0.000000            0.0   0.0       0.0        0.0   \n",
       "2        0.0  ...     0.0  0.000000            0.0   0.0       0.0        0.0   \n",
       "3        0.0  ...     0.0  0.320343            0.0   0.0       0.0        0.0   \n",
       "4        0.0  ...     0.0  0.000000            0.0   0.0       0.0        0.0   \n",
       "5        0.0  ...     0.0  0.000000            0.0   0.0       0.0        0.0   \n",
       "6        0.0  ...     0.0  0.000000            0.0   0.0       0.0        0.0   \n",
       "7        0.0  ...     0.0  0.000000            0.0   0.0       0.0        0.0   \n",
       "8        0.0  ...     0.0  0.000000            0.0   0.0       0.0        0.0   \n",
       "9        0.0  ...     0.0  0.000000            0.0   0.0       0.0        0.0   \n",
       "\n",
       "  unit     using verifiable wanted  \n",
       "0  0.0  0.000000        0.0    0.0  \n",
       "1  0.0  0.000000        0.0    0.0  \n",
       "2  0.0  0.000000        0.0    0.0  \n",
       "3  0.0  0.000000        0.0    0.0  \n",
       "4  0.0  0.000000        0.0    0.0  \n",
       "5  0.0  0.000000        0.0    0.0  \n",
       "6  0.0  0.000000        0.0    0.0  \n",
       "7  0.0  0.000000        0.0    0.0  \n",
       "8  0.0  0.000000        0.0    0.0  \n",
       "9  0.0  0.313317        0.0    0.0  \n",
       "\n",
       "[10 rows x 223 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## For printing that tf-idf matrix, we convert it into dataframe\n",
    "df_tfidf = pd.DataFrame(train_data.toarray(),columns=[tfidf_vectorizer.get_feature_names()])\n",
    "df_tfidf.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before learning the model from training data, please make sure to look back again on previous steps. Was there any overfitting steps performed? Overfitting happens when a model has information of test data. So, please make sure the train and the test dataset should be kept as follows:  \n",
    "\n",
    "<img src=\"https://elitedatascience.com/wp-content/uploads/2017/06/Train-Test-Split-Diagram.jpg\" width=\"500\"> \n",
    "    \n",
    "If yes, please modify the codes and run them again before going further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-04T18:59:36.933470Z",
     "start_time": "2018-11-04T18:59:36.923564Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Classifying the document with BAYES classifier\n",
    "modelbayes = MultinomialNB()\n",
    "modelbayes.fit(train_data,train_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-04T18:59:36.945802Z",
     "start_time": "2018-11-04T18:59:36.937316Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "           metric_params=None, n_jobs=1, n_neighbors=5, p=2,\n",
       "           weights='uniform')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Classifying the document with KNN classifier, k=5\n",
    "modelknn = KNeighborsClassifier(n_neighbors=5)\n",
    "modelknn.fit(train_data,train_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Applying"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-04T18:59:36.955091Z",
     "start_time": "2018-11-04T18:59:36.949073Z"
    }
   },
   "outputs": [],
   "source": [
    "## Perform the same preprocessing for test data\n",
    "test_clean_data = []\n",
    "for test in test_raw:\n",
    "    cleaned_test = clean(test)\n",
    "    cleaned = ' '.join(cleaned_test)\n",
    "    cleaned = re.sub(r\"\\d+\",\"\",cleaned)\n",
    "    test_clean_data.append(cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-04T18:59:36.963591Z",
     "start_time": "2018-11-04T18:59:36.958900Z"
    }
   },
   "outputs": [],
   "source": [
    "## Also perform the tf-idf with the same vectorizer\n",
    "test_data = tfidf_vectorizer.transform(test_clean_data) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-04T18:59:37.005023Z",
     "start_time": "2018-11-04T18:59:36.967541Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>ability</th>\n",
       "      <th>access</th>\n",
       "      <th>action</th>\n",
       "      <th>advance</th>\n",
       "      <th>advanced</th>\n",
       "      <th>advancement</th>\n",
       "      <th>agent</th>\n",
       "      <th>ai</th>\n",
       "      <th>allows</th>\n",
       "      <th>analytical</th>\n",
       "      <th>...</th>\n",
       "      <th>topic</th>\n",
       "      <th>toss</th>\n",
       "      <th>transformation</th>\n",
       "      <th>truly</th>\n",
       "      <th>uncharged</th>\n",
       "      <th>understand</th>\n",
       "      <th>unit</th>\n",
       "      <th>using</th>\n",
       "      <th>verifiable</th>\n",
       "      <th>wanted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 223 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  ability access action advance advanced advancement agent   ai allows  \\\n",
       "0     0.0    0.0    0.0     0.0      0.0         0.0   0.0  0.0    0.0   \n",
       "1     0.0    0.0    0.0     0.0      0.0         0.0   0.0  0.0    0.0   \n",
       "2     0.0    0.0    0.0     0.0      0.0         0.0   0.0  0.0    0.0   \n",
       "\n",
       "  analytical  ...   topic toss transformation truly uncharged understand unit  \\\n",
       "0        0.0  ...     0.0  0.0            0.0   0.0       0.0        0.0  0.0   \n",
       "1        0.0  ...     0.0  0.0            0.0   0.0       0.0        0.0  0.0   \n",
       "2        0.0  ...     0.0  0.0            0.0   0.0       0.0        0.0  0.0   \n",
       "\n",
       "  using verifiable wanted  \n",
       "0   0.0        0.0    0.0  \n",
       "1   0.0        0.0    0.0  \n",
       "2   0.0        0.0    0.0  \n",
       "\n",
       "[3 rows x 223 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## For printing that tf-idf matrix, we convert it into dataframe\n",
    "df_tfidf_test = pd.DataFrame(test_data.toarray(),columns=[tfidf_vectorizer.get_feature_names()])\n",
    "df_tfidf_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-04T18:59:37.013820Z",
     "start_time": "2018-11-04T18:59:37.008686Z"
    }
   },
   "outputs": [],
   "source": [
    "## Applying the model to the test_data\n",
    "predicted_labels_knn = modelknn.predict(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-04T18:59:37.022340Z",
     "start_time": "2018-11-04T18:59:37.017290Z"
    }
   },
   "outputs": [],
   "source": [
    "## Applying the model to the test_data\n",
    "predicted_labels_bayes = modelbayes.predict(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-04T18:59:37.034403Z",
     "start_time": "2018-11-04T18:59:37.025281Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Below 3 test samples will be predicted with KNN :\n",
      "1.  Chemical compunds are used for preparing bombs based on some reactions \n",
      "2.  Cricket is a boring game where the batsman only enjoys the game \n",
      "3.  Machine learning is a area of Artificial intelligence\n",
      "\n",
      "-------PREDICTIONS BY KNN----------\n",
      "\n",
      " Chemical compunds are used for preparing bombs based on some reactions : Chemistry \n",
      " Cricket is a boring game where the batsman only enjoys the game : Cricket \n",
      " Machine learning is a area of Artificial intelligence : AI\n",
      "\n",
      "-------PREDICTIONS BY BAYES CLASSIFIER----------\n",
      "\n",
      " Chemical compunds are used for preparing bombs based on some reactions : Chemistry \n",
      " Cricket is a boring game where the batsman only enjoys the game : Cricket \n",
      " Machine learning is a area of Artificial intelligence : AI\n"
     ]
    }
   ],
   "source": [
    "_labels = ['Cricket','AI','Chemistry']  ## This is for printing\n",
    "\n",
    "print(\"\\nBelow 3 test samples will be predicted with KNN :\\n1. \",\\\n",
    "      test_raw[0],\"\\n2. \",test_raw[1],\"\\n3. \",test_raw[2])\n",
    "\n",
    "print(\"\\n-------PREDICTIONS BY KNN----------\")\n",
    "print(\"\\n\",test_raw[0],\":\",_labels[np.int(predicted_labels_knn[0])],\\\n",
    "       \"\\n\",test_raw[1],\":\",_labels[np.int(predicted_labels_knn[1])],\\\n",
    "       \"\\n\",test_raw[2],\":\",_labels[np.int(predicted_labels_knn[2])])\n",
    "\n",
    "\n",
    "print(\"\\n-------PREDICTIONS BY BAYES CLASSIFIER----------\")\n",
    "print(\"\\n\",test_raw[0],\":\",_labels[np.int(predicted_labels_bayes[0])],\\\n",
    "       \"\\n\",test_raw[1],\":\",_labels[np.int(predicted_labels_bayes[1])],\\\n",
    "       \"\\n\",test_raw[2],\":\",_labels[np.int(predicted_labels_bayes[2])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Real Dataset: 20 Newsgoup \n",
    "\n",
    "The 20 Newsgroups data set is a collection of approximately 20,000 newsgroup documents, partitioned (nearly) evenly across 20 different newsgroups. \n",
    "\n",
    "It can be found at: http://qwone.com/~jason/20Newsgroups/. In SKLEARN, it also was included for research purposes.\n",
    "\n",
    "The dataset in Sklearn is smaller than the original one: it comprises around 18000 newsgroups posts on 20 topics split in two subsets: one for training (or development) and the other one for testing (or for performance evaluation).\n",
    "    \n",
    "You can find another link for downloading 20Newsgroup dataset in UCI Archive (very good place for standard datasets for researching): https://archive.ics.uci.edu/ml/datasets/Twenty+Newsgroups\n",
    "\n",
    "The 20 newsgroups collection has become a popular data set for experiments in text applications of machine learning techniques, such as text classification and text clustering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Loading the data set in jupyter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-04T18:59:37.671165Z",
     "start_time": "2018-11-04T18:59:37.038098Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "twenty_train = fetch_20newsgroups(subset='train', shuffle=True)\n",
    "twenty_test = fetch_20newsgroups(subset='test', shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Extracting features from text files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-04T18:59:37.679887Z",
     "start_time": "2018-11-04T18:59:37.673942Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From: lerxst@wam.umd.edu (where's my thing)\n",
      "Subject: WHAT car is this!?\n",
      "Nntp-Posting-Host: rac3.wam.umd.edu\n"
     ]
    }
   ],
   "source": [
    "twenty_train.target_names #prints all the categories\n",
    "print(\"\\n\".join(twenty_train.data[0].split(\"\\n\")[:3])) #prints first line of the first data file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-04T18:59:41.900929Z",
     "start_time": "2018-11-04T18:59:37.683491Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of train data (count): (11314, 130107)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count_vect = CountVectorizer()\n",
    "X_train_counts = count_vect.fit_transform(twenty_train.data)\n",
    "print('Size of train data (count): ' + str(X_train_counts.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-04T18:59:46.236170Z",
     "start_time": "2018-11-04T18:59:41.907365Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of train data (tfidf): (11314, 130107)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf_transformer = TfidfVectorizer()\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(twenty_train.data)\n",
    "print('Size of train data (tfidf): ' + str(X_train_tfidf.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-04T18:59:46.245845Z",
     "start_time": "2018-11-04T18:59:46.239870Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of labels: 11314\n"
     ]
    }
   ],
   "source": [
    "y_train = twenty_train.target\n",
    "print('Size of labels: ' + str(len(y_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-04T18:59:50.632677Z",
     "start_time": "2018-11-04T18:59:46.251782Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of test data (count): (7532, 130107)\n",
      "Size of test data (tfidf): (7532, 130107)\n"
     ]
    }
   ],
   "source": [
    "X_test_counts = count_vect.transform(twenty_test.data)\n",
    "print('Size of test data (count): ' + str(X_test_counts.shape))\n",
    "X_test_tfidf = tfidf_transformer.transform(twenty_test.data)\n",
    "print('Size of test data (tfidf): ' + str(X_test_tfidf.shape))\n",
    "y_test = twenty_test.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3. Running ML algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-04T19:15:22.264608Z",
     "start_time": "2018-11-04T19:15:20.574455Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 10-fold Cross Validation for Bayes with alpha = 1.0 ----\n",
      "[0.85438596 0.84885764 0.84797891 0.84947183 0.85070671 0.84588131\n",
      " 0.84751773 0.85079929 0.85587189 0.85129118]\n"
     ]
    }
   ],
   "source": [
    "## Cross-validation for checking the accuracy of training steps\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "print('--- 10-fold Cross Validation for Bayes with alpha = 1.0 ----')\n",
    "print(cross_val_score(MultinomialNB(alpha=1.0), X_train_tfidf, y_train, cv=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-04T19:14:53.647535Z",
     "start_time": "2018-11-04T19:14:37.788898Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 10-fold Cross Validation for KNN with k = 5 ----\n",
      "[0.8        0.79613357 0.78558875 0.79929577 0.78091873 0.75819309\n",
      " 0.7712766  0.80106572 0.79181495 0.78539626]\n"
     ]
    }
   ],
   "source": [
    "## Cross-validation for checking the accuracy of training steps\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "print('--- 10-fold Cross Validation for KNN with k = 5 ----')\n",
    "print(cross_val_score(KNeighborsClassifier(n_neighbors=5), X_train_tfidf, y_train, cv=10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span class=\"burk\">What is k-fold Cross-Validation?</span>\n",
    "\n",
    "K -fold Cross-Validation is a technique to evaluate predictive models by partitioning the original sample into a training set to train the model, and a validation set to evaluate it. \n",
    "\n",
    "The below figure show how the 10-folds cross validation works. \n",
    "\n",
    "![\"10-folds Cross Validation\"](https://www.researchgate.net/profile/Juan_Buhagiar2/publication/322509110/figure/fig1/AS:583173118664704@1516050714606/An-example-of-a-10-fold-cross-validation-cro17.png)\n",
    "\n",
    "<span class=\"burk\">When using Cross-Validation?</span>\n",
    "\n",
    "    1) No or lacking of test set.\n",
    "\n",
    "When we have very little data, splitting it into training and test set might leave us with a very small test set. Say we have only 100 examples, if we do a simple 80–20 split, we’ll get 20 examples in our test set. It is not enough. We can get almost any performance on this set only due to chance. The problem is even worse when we have a multi-class problem. If we have 10 classes and only 20 examples, It leaves us with only 2 examples for each class on average. Testing anything on only 2 examples can’t lead to any real conclusion.\n",
    "\n",
    "If we use cross-validation in this case, we build K different models, so we are able to make predictions on all of our data. After we evaluated our learning algorithm we are now can train our model on all our data.\n",
    "\n",
    "    2) Get More Metrics\n",
    "    \n",
    "When we create ten different models using our learning algorithm and test it on ten different test sets, we can be more confident in our algorithm performance. When we do a single evaluation on our test set, we get only one result. This result may be because of chance or a biased test set for some reason. By training five (or ten) different models we can understand better what’s going on. The results also point out how consistent our model can be. So we can be confident that by training it on all the data set and deploy it in production will lead to similar performance.\n",
    "\n",
    "However, there also can have a slightly different scenario, say 91.0, **44.0**, 91.5, 90.5 and 90.8. It looks like one of our folds is from a different distribution. Thus, we have to go back and make sure that our data is what we think it is. The re-check steps here are really important for data analytics to have a clearly understanding to the data. It will make the successful in learning steps.\n",
    "\n",
    "    3)  Parameters Fine-Tuning\n",
    "   \n",
    "This is one of the most common and obvious reasons to do cross validation. Most of the learning algorithms require some parameters tuning.\n",
    "\n",
    "There are many methods to do the tuning optimization. It could be a manual search, a grid search or some more sophisticated optimization. However, for checking the performance of each changing of parameter, in all those cases we can not do it on whole of our training test and definitely not on our test set. We have to use a third set, a validation set.\n",
    "\n",
    "Read more in: http://scikit-learn.org/stable/modules/cross_validation.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span class=\"mark\">Hyperparameter Optimization</span>\n",
    "\n",
    "In machine learning, hyperparameter optimization or tuning is the problem of <span class=\"mark\">choosing a set of optimal hyperparameters</span> for a learning algorithm. Scikit gives an extremely useful tool ‘GridSearchCV’."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For Bayes Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-04T19:21:32.293944Z",
     "start_time": "2018-11-04T19:21:32.289919Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-04T19:21:33.206999Z",
     "start_time": "2018-11-04T19:21:33.201341Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The searching range is: [0.  0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1. ]\n"
     ]
    }
   ],
   "source": [
    "print('The searching range is: ' + str(np.linspace(0,1,11)))\n",
    "clf_val = MultinomialNB()\n",
    "parameters = {'alpha': (np.linspace(0,1,11))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-04T19:22:00.181503Z",
     "start_time": "2018-11-04T19:21:34.062393Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best parameter is:  0.1\n"
     ]
    }
   ],
   "source": [
    "gs_clf = GridSearchCV(clf_val, parameters, cv=10,scoring='accuracy')\n",
    "gs_clf = gs_clf.fit(X_train_tfidf, y_train)\n",
    "NB_parameters = gs_clf.best_params_\n",
    "print('The best parameter is: ',NB_parameters['alpha'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look back to the validation to obtain the good \"alpha\" for Bayes classifier. The below results show how the mean accuracy (plus/minus std) of 10 CV for each dot of alpha in search range. Please note here, the accuracies here are not related to accuracy using testset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-04T19:22:01.017486Z",
     "start_time": "2018-11-04T19:22:01.009902Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[mean: 0.89252, std: 0.01141, params: {'alpha': 0.0},\n",
       " mean: 0.90578, std: 0.00572, params: {'alpha': 0.1},\n",
       " mean: 0.89420, std: 0.00602, params: {'alpha': 0.2},\n",
       " mean: 0.88740, std: 0.00345, params: {'alpha': 0.30000000000000004},\n",
       " mean: 0.88006, std: 0.00365, params: {'alpha': 0.4},\n",
       " mean: 0.87378, std: 0.00338, params: {'alpha': 0.5},\n",
       " mean: 0.86795, std: 0.00387, params: {'alpha': 0.6000000000000001},\n",
       " mean: 0.86389, std: 0.00464, params: {'alpha': 0.7000000000000001},\n",
       " mean: 0.85947, std: 0.00380, params: {'alpha': 0.8},\n",
       " mean: 0.85531, std: 0.00368, params: {'alpha': 0.9},\n",
       " mean: 0.85027, std: 0.00291, params: {'alpha': 1.0}]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_clf.grid_scores_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-04T19:31:53.471109Z",
     "start_time": "2018-11-04T19:31:51.442676Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 10-fold Cross Validation for Bayes with alpha = 0.1 ----\n",
      "[0.9        0.91212654 0.90773286 0.91461268 0.9090106  0.89813995\n",
      " 0.90336879 0.90408526 0.91103203 0.89759573]\n"
     ]
    }
   ],
   "source": [
    "## Cross-validation for checking the accuracy of training steps\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "print('--- 10-fold Cross Validation for Bayes with alpha = 0.1 ----')\n",
    "print(cross_val_score(MultinomialNB(alpha=NB_parameters['alpha']), X_train_tfidf, y_train, cv=10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For KNN Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-04T19:43:25.809712Z",
     "start_time": "2018-11-04T19:43:25.802823Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The searching range is: [1 3 5 7 9]\n"
     ]
    }
   ],
   "source": [
    "print('The searching range is: ' + str(np.arange(1,10,2)))\n",
    "clf_val = KNeighborsClassifier()\n",
    "parameters = {'n_neighbors': (np.arange(1,10,2))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-04T19:55:55.938525Z",
     "start_time": "2018-11-04T19:43:27.484624Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best parameter is:  1\n"
     ]
    }
   ],
   "source": [
    "gs_clf = GridSearchCV(clf_val, parameters, cv=10,scoring='accuracy')\n",
    "gs_clf = gs_clf.fit(X_train_tfidf, y_train)\n",
    "KNN_parameters = gs_clf.best_params_\n",
    "print('The best parameter is: ',KNN_parameters['n_neighbors'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-04T19:57:45.954494Z",
     "start_time": "2018-11-04T19:57:45.944891Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[mean: 0.83481, std: 0.00886, params: {'n_neighbors': 1},\n",
       " mean: 0.79936, std: 0.01336, params: {'n_neighbors': 3},\n",
       " mean: 0.78699, std: 0.01320, params: {'n_neighbors': 5},\n",
       " mean: 0.78018, std: 0.01143, params: {'n_neighbors': 7},\n",
       " mean: 0.77232, std: 0.01194, params: {'n_neighbors': 9}]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_clf.grid_scores_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-04T19:58:18.362721Z",
     "start_time": "2018-11-04T19:58:05.232607Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 10-fold Cross Validation for KNN with k = 1 ----\n",
      "[0.83508772 0.83567663 0.83391916 0.84507042 0.84010601 0.82019486\n",
      " 0.82446809 0.85168739 0.82829181 0.83348175]\n"
     ]
    }
   ],
   "source": [
    "## Cross-validation for checking the accuracy of training steps\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "print('--- 10-fold Cross Validation for KNN with k = 1 ----')\n",
    "print(cross_val_score(KNeighborsClassifier(n_neighbors=KNN_parameters['n_neighbors']), X_train_tfidf, y_train, cv=10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-04T20:09:01.444474Z",
     "start_time": "2018-11-04T20:09:01.276241Z"
    }
   },
   "outputs": [],
   "source": [
    "## Training steps:\n",
    "clf_NB = MultinomialNB(alpha=NB_parameters['alpha']).fit(X_train_tfidf, y_train)\n",
    "clf_KNN = KNeighborsClassifier(n_neighbors=KNN_parameters['n_neighbors']).fit(X_train_tfidf, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-04T20:09:13.924352Z",
     "start_time": "2018-11-04T20:09:02.941962Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for model Naive Bayes (alpha =0.1): 0.8263409453000531\n",
      "Accuracy for model KNN (k=1): 0.6724641529474243\n"
     ]
    }
   ],
   "source": [
    "## Applying steps:\n",
    "predicted_NB = clf_NB.predict(X_test_tfidf)\n",
    "print('Accuracy for model Naive Bayes (alpha =0.1): '+ str(np.mean(predicted_NB == y_test) ))\n",
    "\n",
    "predicted_KNN = clf_KNN.predict(X_test_tfidf)\n",
    "print('Accuracy for model KNN (k=1): '+ str(np.mean(predicted_KNN == y_test) ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recheck with other parameter's setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-04T20:09:47.545351Z",
     "start_time": "2018-11-04T20:09:36.824341Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for model Naive Bayes (alpha =1.0): 0.7738980350504514\n",
      "Accuracy for model KNN (k=5): 0.6591874668082847\n"
     ]
    }
   ],
   "source": [
    "predicted_NB2 = MultinomialNB(alpha=1.0).fit(X_train_tfidf, y_train).predict(X_test_tfidf)\n",
    "print('Accuracy for model Naive Bayes (alpha =1.0): '+ str(np.mean(predicted_NB2 == y_test) ))\n",
    "\n",
    "predicted_KNN2 = KNeighborsClassifier(n_neighbors=5).fit(X_train_tfidf, y_train).predict(X_test_tfidf)\n",
    "print('Accuracy for model KNN (k=5): '+ str(np.mean(predicted_KNN2 == y_test) ))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
