{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COMP30810 - INTRO TO TEXT ANALYTICS\n",
    "> LECTURER: BINH THANH LE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Review Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "solution2": "shown",
    "solution2_first": true
   },
   "source": [
    "## Q1\n",
    "Which of the following techniques can be used for the purpose of keyword normalization, the process of converting a keyword into its base form?\n",
    "\n",
    "    1 - Lemmatization\n",
    "    2 - Levenshtein\n",
    "    3 - Stemming\n",
    "    4 - Soundex\n",
    "\n",
    "A) 1 and 2\n",
    "\n",
    "B) 2 and 4\n",
    "\n",
    "C) 1 and 3\n",
    "\n",
    "D) 1, 2 and 3\n",
    "\n",
    "E) 2, 3 and 4\n",
    "\n",
    "F) 1, 2, 3 and 4\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "solution2": "shown"
   },
   "source": [
    "**Solution: (C)**\n",
    "\n",
    "Lemmatization and stemming are the techniques of keyword normalization, while Levenshtein and Soundex are techniques of string matching."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "solution2": "shown",
    "solution2_first": true
   },
   "source": [
    "## Q2\n",
    "N-grams are defined as the combination of N keywords together. \n",
    "\n",
    "How many bi-grams can be generated from given sentence:\n",
    "\n",
    "**“Analytics Vidhya is a great source to learn data science”**\n",
    "\n",
    "    A) 7\n",
    "    B) 8\n",
    "    C) 9\n",
    "    D) 10\n",
    "    E) 11\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "solution2": "shown"
   },
   "source": [
    "**Solution: (C)**\n",
    "\n",
    "Bigrams: Analytics Vidhya, Vidhya is, is a, a great, great source, source to, To learn, learn data, data science"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "solution2": "shown",
    "solution2_first": true
   },
   "source": [
    "## Q3\n",
    "\n",
    "How many trigrams phrases can be generated from the following sentence, after performing following text cleaning steps:\n",
    "\n",
    " - Stopword Removal\n",
    "\n",
    " - Replacing punctuations by a single space\n",
    "\n",
    "**“#Analytics-vidhya is a great source to learn @data_science.”**\n",
    "\n",
    "    A) 3\n",
    "    B) 4\n",
    "    C) 5\n",
    "    D) 6\n",
    "    E) 7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "solution2": "shown"
   },
   "source": [
    "**Solution: (C)**\n",
    "\n",
    "After performing stopword removal and punctuation replacement the text becomes: \n",
    "\n",
    "**“Analytics vidhya great source learn data science”**\n",
    "\n",
    "Trigrams: Analytics vidhya great, vidhya great source, great source learn, source learn data, learn data science"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "solution2": "shown",
    "solution2_first": true
   },
   "source": [
    "## Q4\n",
    "Which of the following regular expression can be used to identify date(s) present in the text object:\n",
    "\n",
    "**“The next meetup on data science will be held on 2017-09-21, previously it happened on 31/03, 2016”**\n",
    "\n",
    "    A) \\d{4}-\\d{2}-\\d{2}\n",
    "    B) (19|20)\\d{2}-(0[1-9]|1[0-2])-[0-2][1-9] \n",
    "    C) (19|20)\\d{2}-(0[1-9]|1[0-2])-([0-2][1-9]|3[0-1])\n",
    "    D) None of the above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "solution2": "shown"
   },
   "source": [
    "**Solution: (D)**\n",
    "\n",
    "None if these expressions would be able to identify the dates in this text object."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question Context 5-6:\n",
    "\n",
    "You have collected a data of about 10,000 rows of **tweet text** and *no other information*. You want to create a **tweet classification model** that categorizes each of the tweets in three buckets – **positive**, **negative** and **neutral**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "solution2": "shown",
    "solution2_first": true
   },
   "source": [
    "## Q5\n",
    "Which of the following models can perform tweet classification with regards to context mentioned above?\n",
    "\n",
    "    A) Naive Bayes\n",
    "    B) SVM\n",
    "    C) None of the above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "solution2": "shown"
   },
   "source": [
    "**Solution: (C)**\n",
    "\n",
    "Since, you are given only the data of tweets and no other information, which means there is no target variable present. One cannot train a supervised learning model, both **svm and naive bayes are supervised learning techniques**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "solution2": "shown",
    "solution2_first": true
   },
   "source": [
    "## Q6\n",
    "You have created a **document term matrix** of the data, treating *every tweet as one document*. Which of the following is correct, in regards to document term matrix?\n",
    "\n",
    "1) Removal of stopwords from the data will affect the dimensionality of data\n",
    "\n",
    "2) Normalization of words in the data will reduce the dimensionality of data\n",
    "\n",
    "3) Converting all the words in lowercase will not affect the dimensionality of the data\n",
    "\n",
    "    A) Only 1\n",
    "    B) Only 2\n",
    "    C) Only 3\n",
    "    D) 1 and 2\n",
    "    E) 2 and 3\n",
    "    F) 1, 2 and 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "solution2": "shown"
   },
   "source": [
    "**Solution: (D)**\n",
    "\n",
    "Choices A and B are correct because stopword removal will decrease the number of features in the matrix, normalization of words will also reduce redundant features, and, converting all words to lowercase will also decrease the dimensionality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "solution2": "shown",
    "solution2_first": true
   },
   "source": [
    "## Q7\n",
    "\n",
    "Which of the following features can be used for accuracy improvement of a classification model?\n",
    "\n",
    "    A) Frequency count of terms\n",
    "    B) Part of Speech Tag\n",
    "    C) Dependency Grammar\n",
    "    D) All of these\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "solution2": "shown"
   },
   "source": [
    "**Solution: (D)**\n",
    "\n",
    "All of the techniques can be used for the purpose of engineering features in a model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "solution2": "shown",
    "solution2_first": true
   },
   "source": [
    "## Q8\n",
    "Solve the equation according to the sentence \n",
    "\n",
    "**“I am planning to visit New Delhi to attend Analytics Vidhya Delhi Hackathon”**.\n",
    "\n",
    "    A = (# of words with Noun as the part of speech tag)\n",
    "    B = (# of words with Verb as the part of speech tag)\n",
    "    C = (# of words with frequency count greater than one)\n",
    "\n",
    "What are the correct values of A, B, and C?\n",
    "\n",
    "    A) 5, 5, 2\n",
    "    B) 5, 5, 0\n",
    "    C) 7, 5, 1\n",
    "    D) 7, 4, 2\n",
    "    E) 6, 4, 3\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "solution2": "shown"
   },
   "source": [
    "**Solution: (D)**\n",
    "\n",
    "    Nouns: I, New, Delhi, Analytics, Vidhya, Delhi, Hackathon (7)\n",
    "\n",
    "    Verbs: am, planning, visit, attend (4)\n",
    "\n",
    "    Words with frequency counts > 1: to, Delhi (2)\n",
    "\n",
    "Hence option D is correct."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "solution2": "shown",
    "solution2_first": true
   },
   "source": [
    "## Q9\n",
    "\n",
    "In a corpus of N documents, one document is randomly picked. The document contains a total of T terms and the term “data” appears K times.\n",
    "\n",
    "What is the correct value for the product of TF (term frequency) and IDF (inverse-document-frequency), if the term “data” appears in approximately one-third of the total documents?\n",
    "\n",
    "    A) KT * Log(3)\n",
    "    B) K * Log(3) / T\n",
    "    C) T * Log(3) / K\n",
    "    D) Log(3) / KT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "solution2": "shown"
   },
   "source": [
    "**Solution: (B)**\n",
    "\n",
    "Formula for TF is **K/T**\n",
    "\n",
    "Formula for IDF is **log(total docs / no of docs containing “data”)**\n",
    "\n",
    "    = log(1 / (⅓))\n",
    "\n",
    "    = log (3)\n",
    "\n",
    "Hence correct choice is **K*Log(3)/T**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question Context 10 to 12:\n",
    "Refer the following document term matrix\n",
    "\n",
    "<img src=\"Picture1.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "solution2": "shown",
    "solution2_first": true
   },
   "source": [
    "## Q10\n",
    "Which of the following documents contains the same number of terms and the number of terms in the one of the document is not equal to least number of terms in any document in the entire corpus.\n",
    "\n",
    "    A) d1 and d4\n",
    "    B) d6 and d7\n",
    "    C) d2 and d4\n",
    "    D) d5 and d6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "solution2": "shown"
   },
   "source": [
    "**Solution: (C)**\n",
    "\n",
    "Both of the documents d2 and d4 contains 4 terms and does not contain the least number of terms which is 3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "solution2": "shown",
    "solution2_first": true
   },
   "source": [
    "## Q11\n",
    "Which are the most common and the rarest term of the corpus?\n",
    "\n",
    "    A) t4, t6\n",
    "    B) t3, t5\n",
    "    C) t5, t1\n",
    "    D) t5, t6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "solution2": "shown"
   },
   "source": [
    "**Solution: (A)**\n",
    "\n",
    "T5 is most common terms across 5 out of 7 documents, T6 is rare term only appears in d3 and d4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "solution2": "shown",
    "solution2_first": true
   },
   "source": [
    "## Q12\n",
    "<img src=\"Picture1.png\">\n",
    "\n",
    "What is the term frequency of a term which is used a maximum number of times in that document?\n",
    "\n",
    "    A) t6 – 2/5\n",
    "    B) t3 – 3/6\n",
    "    C) t4 – 2/6\n",
    "    D) t1 – 2/6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "solution2": "shown"
   },
   "source": [
    "**Solution: (B)**\n",
    "\n",
    "t3 is used max times in entire corpus = 3, tf for t3 is 3/6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "solution2": "hidden",
    "solution2_first": true
   },
   "source": [
    "## Q13\n",
    "Which of the following models can be used for the purpose of document similarity?\n",
    "\n",
    "    A) Training a word 2 vector model on the corpus that learns context present in the document\n",
    "    B) Training a bag of words model that learns occurrence of words in the document\n",
    "    C) Creating a document-term matrix and using cosine similarity for each document\n",
    "    D) All of the above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "solution2": "hidden"
   },
   "source": [
    "**Solution: (D)**\n",
    "\n",
    "word2vec model can be used for measuring document similarity based on context. Bag Of Words and document term matrix can be used for measuring similarity based on terms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "solution2": "hidden",
    "solution2_first": true
   },
   "source": [
    "## Q14\n",
    "\n",
    "What are the possible features of a text corpus\n",
    "\n",
    "    1 - Count of word in a document\n",
    "    2 - Boolean feature – presence of word in a document\n",
    "    3 - Vector notation of word\n",
    "    4 - Part of Speech Tag\n",
    "    5 - Basic Dependency Grammar\n",
    "    6 - Entire document as a feature\n",
    "\n",
    "        A) 1\n",
    "        B) 12\n",
    "        C) 123\n",
    "        D) 1234\n",
    "        E) 12345\n",
    "        F) 123456"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "solution2": "hidden"
   },
   "source": [
    "**Solution: (E)**\n",
    "\n",
    "Except for entire document as the feature, rest all can be used as features of text classification learning model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "solution2": "hidden",
    "solution2_first": true
   },
   "source": [
    "## Q15\n",
    "\n",
    "Google Search’s feature – **“Did you mean”**, is a mixture of different techniques. Which of the following techniques are likely to be ingredients?\n",
    "\n",
    "    1 - Collaborative Filtering model to detect similar user behaviors (queries)\n",
    "    2 - Model that checks for Levenshtein distance among the dictionary terms\n",
    "    3 - Translation of sentences into multiple languages\n",
    "    \n",
    "            A) 1\n",
    "            B) 2\n",
    "            C) 1, 2\n",
    "            D) 1, 2, 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "solution2": "hidden"
   },
   "source": [
    "**Solution: (C)**\n",
    "\n",
    "Collaborative filtering can be used to check what are the patterns used by people, Levenshtein is used to measure the distance among dictionary terms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "solution2": "hidden",
    "solution2_first": true
   },
   "source": [
    "## Q16\n",
    "\n",
    "Social Media platforms are the most intuitive form of text data. You are given a corpus of complete social media data of tweets. How can you create a model that suggests the hashtags?\n",
    "\n",
    "    A) Perform Topic Models to obtain most significant words of the corpus\n",
    "    B) Train a Bag of Ngrams model to capture top n-grams – words and their combinations\n",
    "    C) Train a word2vector model to learn repeating contexts in the sentences\n",
    "    D) All of these"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "solution2": "hidden"
   },
   "source": [
    "**Solution: (D)**\n",
    "\n",
    "All of the techniques can be used to extract most significant terms of a corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "solution2": "hidden",
    "solution2_first": true
   },
   "source": [
    "## Q17\n",
    "\n",
    "Movie Recommendation systems are an example of:\n",
    "\n",
    "    1 - Classification\n",
    "    2 - Clustering\n",
    "    3 - Reinforcement Learning\n",
    "    4 - Regression\n",
    "\n",
    "        A. 1 Only\n",
    "        B. 2 Only\n",
    "        C. 1 and 2\n",
    "        D. 1 and 3\n",
    "        E. 2 and 3\n",
    "        F. 1, 2 and 3\n",
    "        H. 1, 2, 3 and 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "solution2": "hidden"
   },
   "source": [
    "**Solution: (E)**\n",
    "\n",
    "Generally, movie recommendation systems cluster the users in a finite number of similar groups based on their previous activities and profile. Then, at a fundamental level, people in the same cluster are made similar recommendations.\n",
    "\n",
    "In some scenarios, this can also be approached as a classification problem for assigning the most appropriate movie class to the user of a specific group of users. Also, a movie recommendation system can be viewed as a reinforcement learning problem where it learns by its previous recommendations and improves the future recommendations.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "solution2": "hidden",
    "solution2_first": true
   },
   "source": [
    "## Q18\n",
    "\n",
    "Sentiment Analysis is an example of:\n",
    "\n",
    "    1 - Regression\n",
    "    2 - Classification\n",
    "    3 - Clustering\n",
    "    4 - Reinforcement Learning\n",
    "\n",
    "            A. 1 Only\n",
    "            B. 1 and 2\n",
    "            C. 1 and 3\n",
    "            D. 1, 2 and 3\n",
    "            E. 1, 2 and 4\n",
    "            F. 1, 2, 3 and 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "solution2": "hidden"
   },
   "source": [
    "**Solution: (E)**\n",
    "\n",
    "Sentiment analysis at the fundamental level is the task of classifying the sentiments represented in an image, text or speech into a set of defined sentiment classes like happy, sad, excited, positive, negative, etc. It can also be viewed as a regression problem for assigning a sentiment score of say 1 to 10 for a corresponding image, text or speech.\n",
    "\n",
    "Another way of looking at sentiment analysis is to consider it using a reinforcement learning perspective where the algorithm constantly learns from the accuracy of past sentiment analysis performed to improve the future performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "solution2": "hidden",
    "solution2_first": true
   },
   "source": [
    "## Q19\n",
    "\n",
    "For two runs of K-Mean clustering is it expected to get same clustering results?\n",
    "\n",
    "    A. Yes\n",
    "    B. No"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "solution2": "hidden"
   },
   "source": [
    "**Solution: (B)**\n",
    "\n",
    "K-Means clustering algorithm instead converses on local minima which might also correspond to the global minima in some cases but not always. Therefore, it’s advised to run the K-Means algorithm multiple times before drawing inferences about the clusters.\n",
    "\n",
    "However, note that it’s possible to receive same clustering results from K-means by setting the same seed value for each run. But that is done by simply making the algorithm choose the set of same random no. for each run."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "solution2": "hidden",
    "solution2_first": true
   },
   "source": [
    "## Q20\n",
    "\n",
    "What is the most appropriate no. of clusters for the data points represented by the following dendrogram:\n",
    "\n",
    "<img src=\"Picture2.png\">\n",
    "\n",
    "    A. 2\n",
    "    B. 4\n",
    "    C. 6\n",
    "    D. 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "solution2": "hidden"
   },
   "source": [
    "**Solution: (B)**\n",
    "\n",
    "The decision of the no. of clusters that can best depict different groups can be chosen by observing the dendrogram. The best choice of the no. of clusters is the no. of vertical lines in the dendrogram cut by a horizontal line that can transverse the maximum distance vertically without intersecting a cluster.\n",
    "\n",
    "<img src='Picture3.png'>\n",
    "\n",
    "In the above example, the best choice of no. of clusters will be 4 as the red horizontal line in the dendrogram below covers maximum vertical distance AB."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "solution2": "hidden",
    "solution2_first": true
   },
   "source": [
    "## Q21\n",
    "\n",
    "In which of the following cases will K-Means clustering fail to give good results?\n",
    "\n",
    "    1 - Data points with outliers\n",
    "    2 - Data points with different densities\n",
    "    3 - Data points with round shapes\n",
    "    4 - Data points with non-convex shapes\n",
    "\n",
    "Options:\n",
    "\n",
    "            A. 1 and 2\n",
    "            B. 2 and 3\n",
    "            C. 2 and 4\n",
    "            D. 1, 2 and 4\n",
    "            E. 1, 2, 3 and 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "solution2": "hidden"
   },
   "source": [
    "**Solution: (D)**\n",
    "\n",
    "K-Means clustering algorithm fails to give good results when the data contains outliers, the density spread of data points across the data space is different and the data points follow non-convex shapes.\n",
    "\n",
    "<img src='Picture4.png'>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "solution2": "hidden",
    "solution2_first": true
   },
   "source": [
    "## Q22\n",
    "Which of the following are the high and low bounds for the existence of F-Score?\n",
    "\n",
    "    A. [0,1]\n",
    "    B. (0,1)\n",
    "    C. [-1,1]\n",
    "    D. None of the above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "solution2": "hidden"
   },
   "source": [
    "**Solution: (A)**\n",
    "\n",
    "The lowest and highest possible values of F score are 0 and 1 with 1 representing that every data point is assigned to the correct cluster and 0 representing that the precession and/ or recall of the clustering analysis are both 0. In clustering analysis, high value of F score is desired."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "solution2": "shown",
    "solution2_first": true
   },
   "source": [
    "## Q23\n",
    "\n",
    "Following are the results observed for clustering 6000 data points into 3 clusters: A, B and C:\n",
    "\n",
    "\n",
    "\n",
    "What is the F1-Score with respect to cluster B?\n",
    "\n",
    "    A. 0.3\n",
    "    B. 0.4\n",
    "    C. 0.5\n",
    "    D. 0.6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "solution2": "shown"
   },
   "source": [
    "**Solution: (D)**\n",
    "\n",
    "Here,\n",
    "\n",
    "<img src='Picture5.png'>\n",
    "\n",
    "True Positive, **TP = 1200**\n",
    "\n",
    "True Negative, **TN = 600 + 1600 = 2200**\n",
    "\n",
    "False Positive, **FP = 1000 + 200 = 1200**\n",
    "\n",
    "False Negative, **FN = 400 + 400 = 800**\n",
    "\n",
    "Therefore,\n",
    "\n",
    "**Precision** = TP / (TP + FP) = 0.5\n",
    "\n",
    "**Recall** = TP / (TP + FN) = 0.6\n",
    "\n",
    "Hence,\n",
    "\n",
    "**F1  = 2 * (Precision * Recall)/ (Precision + recall)** = 0.54 ~ 0.5"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
